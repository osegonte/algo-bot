#!/usr/bin/env python3
"""
Level 7-B: Baseline Model Training (FIXED)
Train/test split on historical features, predict next-day P&L or win rate
"""

import pandas as pd
import numpy as np
import json
import pickle
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ML libraries
try:
    from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    ML_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è scikit-learn not available. Install with: pip install scikit-learn")
    ML_AVAILABLE = False

class MLModelTrainer:
    """ML model trainer for strategy performance prediction"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        
        # Directories
        self.features_dir = Path("data/features")
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        # Model configuration
        self.model_config = self.config.get('ml_models', {
            'primary': 'gradient_boosting',
            'target_variable': 'target_pnl',
            'test_size': 0.2,
            'random_state': 42,
            'cv_folds': 5
        })
        
        # Feature selection
        self.feature_blacklist = [
            'date', 'timestamp', 'child_id', 'strategy',
            'date_parsed'  # Derived datetime
        ]
        
        # Models
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        
    def load_feature_data(self, file_pattern: str = "ml_features_*.csv") -> pd.DataFrame:
        """Load feature dataset for training"""
        
        feature_files = list(self.features_dir.glob(file_pattern))
        
        if not feature_files:
            raise FileNotFoundError(f"No feature files found matching {file_pattern}")
        
        # Load most recent feature file
        latest_file = max(feature_files, key=lambda f: f.stat().st_mtime)
        
        print(f"üìä Loading features from: {latest_file.name}")
        
        df = pd.read_csv(latest_file)
        
        print(f"   Rows: {len(df)}")
        print(f"   Features: {len(df.columns)}")
        print(f"   Strategies: {df['strategy'].nunique() if 'strategy' in df.columns else 'N/A'}")
        
        return df
    
    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, List[str]]:
        """Prepare features and target for training"""
        
        # Separate features and target
        target_col = self.model_config['target_variable']
        
        if target_col not in df.columns:
            raise ValueError(f"Target variable '{target_col}' not found in dataset")
        
        # Remove blacklisted features
        feature_cols = [col for col in df.columns if col not in self.feature_blacklist + [target_col]]
        
        # Handle target variables
        target_features = [col for col in df.columns if col.startswith('target_')]
        feature_cols = [col for col in feature_cols if col not in target_features or col == target_col]
        
        print(f"üéØ Target variable: {target_col}")
        print(f"üîß Feature columns: {len(feature_cols)}")
        
        # Extract features and target
        X = df[feature_cols].copy()
        y = df[target_col].values
        
        # Handle missing values
        X = X.fillna(0)
        
        # Remove any remaining non-numeric columns
        numeric_features = []
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64', 'bool']:
                numeric_features.append(col)
            else:
                print(f"‚ö†Ô∏è Skipping non-numeric feature: {col}")
        
        X = X[numeric_features]
        
        print(f"‚úÖ Final dataset: {X.shape[0]} samples √ó {X.shape[1]} features")
        print(f"üìä Target statistics: mean={np.mean(y):.2f}, std={np.std(y):.2f}")
        
        return X, y, numeric_features
    
    def create_models(self) -> Dict:
        """Create model instances for training"""
        
        random_state = self.model_config.get('random_state', 42)
        
        models = {
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=random_state,
                subsample=0.8
            ),
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=random_state,
                n_jobs=-1
            ),
            'linear_regression': LinearRegression(),
            'ridge_regression': Ridge(alpha=1.0, random_state=random_state)
        }
        
        return models
    
    def train_models(self, df: pd.DataFrame) -> Dict:
        """Train multiple models and select best performer"""
        
        if not ML_AVAILABLE:
            raise ImportError("scikit-learn is required for model training")
        
        print("ü§ñ Training ML Models...")
        print("=" * 40)
        
        # Prepare data
        X, y, feature_names = self.prepare_training_data(df)
        
        # Time-based split for time series data
        if 'date' in df.columns:
            # Sort by date to ensure proper time series split
            df_sorted = df.sort_values('date')
            X_sorted, y_sorted, _ = self.prepare_training_data(df_sorted)
            
            # Use time series split (70% train, 30% test)
            split_point = int(len(X_sorted) * 0.7)
            X_train, X_test = X_sorted[:split_point], X_sorted[split_point:]
            y_train, y_test = y_sorted[:split_point], y_sorted[split_point:]
            
            print(f"üìÖ Time series split: {len(X_train)} train, {len(X_test)} test")
        else:
            # Random split if no date column
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.model_config['test_size'],
                random_state=self.model_config['random_state']
            )
            print(f"üé≤ Random split: {len(X_train)} train, {len(X_test)} test")
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Create models
        models = self.create_models()
        
        # Training results
        training_results = {}
        
        # Train each model
        for model_name, model in models.items():
            print(f"\nüîß Training {model_name}...")
            
            try:
                # Train model
                model.fit(X_train_scaled, y_train)
                
                # Predictions
                train_pred = model.predict(X_train_scaled)
                test_pred = model.predict(X_test_scaled)
                
                # Metrics
                train_mae = mean_absolute_error(y_train, train_pred)
                test_mae = mean_absolute_error(y_test, test_pred)
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                
                # Cross-validation
                cv_scores = cross_val_score(
                    model, X_train_scaled, y_train, 
                    cv=self.model_config['cv_folds'], 
                    scoring='neg_mean_absolute_error'
                )
                cv_mae = -cv_scores.mean()
                
                # Store results
                results = {
                    'model': model,
                    'train_mae': train_mae,
                    'test_mae': test_mae,
                    'train_r2': train_r2,
                    'test_r2': test_r2,
                    'cv_mae': cv_mae,
                    'cv_std': cv_scores.std(),
                    'feature_count': len(feature_names)
                }
                
                # Feature importance (if available)
                if hasattr(model, 'feature_importances_'):
                    importance = model.feature_importances_
                    feature_importance = dict(zip(feature_names, importance))
                    results['feature_importance'] = feature_importance
                
                training_results[model_name] = results
                
                print(f"   Train MAE: {train_mae:.3f}")
                print(f"   Test MAE: {test_mae:.3f}")
                print(f"   Test R¬≤: {test_r2:.3f}")
                print(f"   CV MAE: {cv_mae:.3f} ¬± {cv_scores.std():.3f}")
                
            except Exception as e:
                print(f"   ‚ùå Training failed: {e}")
                training_results[model_name] = {'error': str(e)}
        
        # Select best model
        valid_results = {k: v for k, v in training_results.items() if 'error' not in v}
        
        if valid_results:
            # Choose model with lowest test MAE
            best_model_name = min(valid_results.keys(), key=lambda k: valid_results[k]['test_mae'])
            best_model_info = valid_results[best_model_name]
            
            print(f"\nüèÜ Best Model: {best_model_name}")
            print(f"   Test MAE: {best_model_info['test_mae']:.3f}")
            print(f"   Test R¬≤: {best_model_info['test_r2']:.3f}")
            
            # Save best model and scaler
            self.models['best'] = best_model_info['model']
            self.scalers['best'] = scaler
            self.feature_importance['best'] = best_model_info.get('feature_importance', {})
            
            # Save model artifacts
            model_path = self._save_model_artifacts(
                best_model_info['model'], scaler, feature_names, 
                best_model_name, best_model_info
            )
            
            # Update results with save path
            training_results['model_metadata'] = {
                'best_model': best_model_name,
                'model_path': str(model_path),
                'feature_names': feature_names,
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'trained_at': datetime.now(timezone.utc).isoformat()
            }
            
        else:
            print("‚ùå No models trained successfully")
        
        return training_results
    
    def _save_model_artifacts(self, model, scaler, feature_names: List[str], 
                            model_name: str, model_info: Dict) -> Path:
        """Save trained model and metadata"""
        
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        
        # Model artifacts
        artifacts = {
            'model': model,
            'scaler': scaler,
            'feature_names': feature_names,
            'model_name': model_name,
            'performance': {
                'test_mae': model_info['test_mae'],
                'test_r2': model_info['test_r2'],
                'cv_mae': model_info['cv_mae']
            },
            'feature_importance': model_info.get('feature_importance', {}),
            'trained_at': datetime.now(timezone.utc).isoformat(),
            'model_version': '1.0'
        }
        
        # Save model
        model_file = self.models_dir / f"ml_model_{model_name}_{timestamp}.pkl"
        with open(model_file, 'wb') as f:
            pickle.dump(artifacts, f)
        
        # Create symlink to latest
        latest_model = self.models_dir / "latest_model.pkl"
        if latest_model.exists():
            latest_model.unlink()
        latest_model.symlink_to(model_file.name)
        
        # Save human-readable metadata
        metadata = {
            'model_name': model_name,
            'model_file': str(model_file),
            'performance': artifacts['performance'],
            'feature_count': len(feature_names),
            'trained_at': artifacts['trained_at'],
            'top_features': self._get_top_features(model_info.get('feature_importance', {}))
        }
        
        metadata_file = self.models_dir / f"model_metadata_{timestamp}.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"üíæ Model saved: {model_file.name}")
        print(f"üìã Metadata: {metadata_file.name}")
        
        return model_file
    
    def _get_top_features(self, feature_importance: Dict, top_n: int = 10) -> List[Dict]:
        """Get top N most important features"""
        
        if not feature_importance:
            return []
        
        # Sort by importance
        sorted_features = sorted(
            feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return [
            {'feature': name, 'importance': float(importance)}
            for name, importance in sorted_features[:top_n]
        ]

def main():
    """Command line interface for model training"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description="ML Model Training (Level 7-B)")
    parser.add_argument('--features', help='Feature file pattern (default: ml_features_*.csv)')
    parser.add_argument('--target', default='target_pnl', help='Target variable to predict')
    
    args = parser.parse_args()
    
    print("ü§ñ ML Model Training System (Level 7-B)")
    print("=" * 50)
    
    if not ML_AVAILABLE:
        print("‚ùå scikit-learn not available")
        print("   Install with: pip install scikit-learn")
        return
    
    # Initialize trainer
    config = {'ml_models': {'target_variable': args.target}}
    trainer = MLModelTrainer(config)
    
    try:
        # Load feature data
        pattern = args.features or "ml_features_*.csv"
        df = trainer.load_feature_data(pattern)
        
        # Train new models
        results = trainer.train_models(df)
        
        if 'model_metadata' in results:
            print(f"\n‚úÖ Level 7-B Complete!")
            print(f"ü§ñ Best model: {results['model_metadata']['best_model']}")
            print(f"üìä Training samples: {results['model_metadata']['training_samples']}")
            print(f"üéØ Ready for Level 7-C: Live Inference")
        else:
            print("‚ùå No models trained successfully")
    
    except FileNotFoundError as e:
        print(f"‚ùå {e}")
        print("üí° Run Level 7-A first to generate feature data")
    except Exception as e:
        print(f"‚ùå Training failed: {e}")

if __name__ == "__main__":
    main()